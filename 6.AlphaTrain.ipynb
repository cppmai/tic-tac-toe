{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    " \n",
    "    # encode state into model \n",
    "    # output: 3 planes\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module: base for all neural network modules\n",
    "# Modules can contain Module, allow to nest them in tree structure\n",
    "class ResNet(nn.Module):\n",
    "    # num_resBlocks: the length of our blackbone\n",
    "    # num_hidden: the hidden size \n",
    "    def __init__(self, game, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "        # nn.Sequential: models are added in the order they passed in the constructor\n",
    "        # chains outputs to inputs sequentially for each subsequenr module\n",
    "        # finally return uput of last module\n",
    "        \n",
    "        # nn.Conv2d: extract features from an input image \n",
    "        # in_channels: 3 planes \n",
    "        # out_channels: num_hidden \n",
    "        # kernel_size=3, padding=1: won't actually change the shape of our game\n",
    "        \n",
    "        # BatchNorm2d: transformation maintaing mean output ~ 0 and ouput SD ~ 1\n",
    "        # increase the speed for us to train\n",
    "        \n",
    "        # reLu(x) = max(0,x)\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # array of different resBlock \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        # 32 planes\n",
    "        # linear: \n",
    "        # input: tổng tất cả các ô trên 32 planes đó\n",
    "        # ouput: action_size \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        # ouput: 1 neuron \n",
    "        # nn.Tanh: define output in range [-1, 1]\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3 planes -> convolution -> batch normalize -> reLu \n",
    "        # output: 3 processed matrix (same shape as game)\n",
    "        x = self.startBlock(x)\n",
    "        \n",
    "        # loop through residual blocks \n",
    "        # out: hidden_layers matrix \n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "# residual block     \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "    \n",
    "    # skip connection \n",
    "    # extract pattern from matrix\n",
    "    # x -> weight layer -> reLu -> F(x) -> weight layer -> x + F(x) -> reLu   \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797c3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852449893951416\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        # get from parent when we expand \n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        # we don't immediately visit nodes during expanding\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        # loop all policies \n",
    "        # expand all possible directions \n",
    "        for action, prob in enumerate(policy):\n",
    "            # prob=0: expanded before \n",
    "            if prob > 0:\n",
    "                # create new child \n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "#    2 keys changes: \n",
    "   \n",
    "#    - reach leaf -> predict policy of child states and value for current state -> expand) \n",
    "#    - update accessors\n",
    "#    - use UCB to choose child \n",
    "   \n",
    "#    - use value from model to backpropagate -> remove simulation part\n",
    "    \n",
    "    # we don't want to use policy for training immediately -> don't need gradient\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                # input: 3 encoded planes from state game\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                # just use for prediction\n",
    "                # don't want to store the value of gradient of this tensor\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                # ilegal move will have policy value = 0 \n",
    "                policy *= valid_moves\n",
    "                # rescale policies \n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                # just want to have one float value \n",
    "                value = value.item()\n",
    "                \n",
    "                # use policy for expansion\n",
    "                node.expand(policy)\n",
    "            \n",
    "            # use value for backpropagation \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    " \n",
    " # 2 components: the save part and the training part\n",
    " \n",
    "    def selfPlay(self):\n",
    "        # memory inside of 1 own self-play game\n",
    "        memory = []\n",
    "        player = 1\n",
    "        # define an initial state\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            # MCTS base on current state -> p & v -> sample action from distribution of action probability -> use that action to play \n",
    "            # -> new state -> is the state is terminal -> end: return all of states to memory \n",
    "            # data store in tuples form: (state, action probabilities of MCTS, final outcome)\n",
    "            \n",
    "            # when call mcts.search, we always are player 1 -> get neutral state \n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                # loop memory \n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # hist_player is first player ?\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        # shuffle training data, not get the same batches all the time \n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        # loop over all of memory \n",
    "        # for each vatchIndex, sample a whole batch of # samples -> train \n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            # take a sample from memory\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            # transpose our sample: list of tuples -> list of states, list of policies, list of targets \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            # change into numpy array\n",
    "            # .reshape(-1, 1): list of float values -> each of these value actually in own sub-array --> easy to compare with the output of model\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            # change into tensors \n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            # get policy, value from model by letting it predict the state \n",
    "            # purpose: reduce the different between (policy, mcst-distribution) and (value, final reward)\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            # need to minimize this loss by backpropagation \n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # pytorch does all big propagation for us \n",
    "            # instead of setting to 0, set the grads to None -> improve performance\n",
    "            # did not receive a gradient \n",
    "            self.optimizer.zero_grad()\n",
    "            # the gradients are stored by tensor themselves once call backward() on loss\n",
    "            # accumulates the gradient for each parameter \n",
    "            loss.backward()\n",
    "            # makes the optimiser iterate over all tensors -> update and use internally store grad to update their values \n",
    "            self.optimizer.step() \n",
    "    \n",
    "    # use when we want to start the cycle of continuous learning\n",
    "    # where we run sort of play -> get data -> use data to train and optimize model\n",
    "    # close if play agian with the model that is much smarter \n",
    "    def learn(self):\n",
    "        # loop all iterations \n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            # each iter: store training data \n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            # loop over all self-play games\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                # for each iter -> extend data got out of selfPlay method \n",
    "                memory += self.selfPlay()\n",
    "            \n",
    "            # training part\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # store weights (learnable para) of our model, a dictionary object that maps each layer to its parameter tensor \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            # store static of optimizer \n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50768720c995414aa713b50019b2cf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f967471f6f224187a4941ca668f653f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4df25184a84cda8e1a00b404440fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759d21dd81243ef8df560804053fcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d22a5bdf445258313460162b19bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25e187f4e73459b8ec3468b967da3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c470145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [0, 1, 3, 5, 6, 7, 8]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "valid_moves [0, 1, 3, 6, 7]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0. -1.  1.]\n",
      " [ 1.  0. -1.]]\n",
      "[[-1.  0.  1.]\n",
      " [ 0. -1.  1.]\n",
      " [ 1.  0. -1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
