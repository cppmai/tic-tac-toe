{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    " \n",
    "    # encode state into model \n",
    "    # output: 3 planes\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module: base for all neural network modules\n",
    "# Modules can contain Module, allow to nest them in tree structure\n",
    "class ResNet(nn.Module):\n",
    "    # num_resBlocks: the length of our blackbone\n",
    "    # num_hidden: the hidden size \n",
    "    def __init__(self, game, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "        # nn.Sequential: models are added in the order they passed in the constructor\n",
    "        # chains outputs to inputs sequentially for each subsequenr module\n",
    "        # finally return uput of last module\n",
    "        \n",
    "        # nn.Conv2d: extract features from an input image \n",
    "        # in_channels: 3 planes \n",
    "        # out_channels: num_hidden \n",
    "        # kernel_size=3, padding=1: won't actually change the shape of our game\n",
    "        \n",
    "        # BatchNorm2d: transformation maintaing mean output ~ 0 and ouput SD ~ 1\n",
    "        # increase the speed for us to train\n",
    "        \n",
    "        # reLu(x) = max(0,x)\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # array of different resBlock \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        # 32 planes\n",
    "        # linear: \n",
    "        # input: tổng tất cả các ô trên 32 planes đó\n",
    "        # ouput: action_size \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        # ouput: 1 neuron \n",
    "        # nn.Tanh: define output in range [-1, 1]\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3 planes -> convolution -> batch normalize -> reLu \n",
    "        # output: 3 processed matrix (same shape as game)\n",
    "        x = self.startBlock(x)\n",
    "        \n",
    "        # loop through residual blocks \n",
    "        # out: hidden_layers matrix \n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "# residual block     \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "    \n",
    "    # skip connection \n",
    "    # extract pattern from matrix\n",
    "    # x -> weight layer -> reLu -> F(x) -> weight layer -> x + F(x) -> reLu   \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797c3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9062723517417908\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        # get from parent when we expand \n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        # we don't immediately visit nodes during expanding\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        # loop all policies \n",
    "        # expand all possible directions \n",
    "        for action, prob in enumerate(policy):\n",
    "            # prob=0: expanded before \n",
    "            if prob > 0:\n",
    "                # create new child \n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "#    2 keys changes: \n",
    "   \n",
    "#    - reach leaf -> predict policy of child states and value for current state -> expand) \n",
    "#    - update accessors\n",
    "#    - use UCB to choose child \n",
    "   \n",
    "#    - use value from model to backpropagate -> remove simulation part\n",
    "    \n",
    "    # we don't want to use policy for training immediately -> don't need gradient\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                # input: 3 encoded planes from state game\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                # just use for prediction\n",
    "                # don't want to store the value of gradient of this tensor\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                # ilegal move will have policy value = 0 \n",
    "                policy *= valid_moves\n",
    "                # rescale policies \n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                # just want to have one float value \n",
    "                value = value.item()\n",
    "                \n",
    "                # use policy for expansion\n",
    "                node.expand(policy)\n",
    "            \n",
    "            # use value for backpropagation \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    " \n",
    " # 2 components: the save part and the training part\n",
    " \n",
    "    def selfPlay(self):\n",
    "        # memory inside of 1 own self-play game\n",
    "        memory = []\n",
    "        player = 1\n",
    "        # define an initial state\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            # MCTS base on current state -> p & v -> sample action from distribution of action probability -> use that action to play \n",
    "            # -> new state -> is the state is terminal -> end: return all of states to memory \n",
    "            # data store in tuples form: (state, action probabilities of MCTS, final outcome)\n",
    "            \n",
    "            # when call mcts.search, we always are player 1 -> get neutral state \n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                # loop memory \n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # hist_player is first player ?\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        # shuffle training data, not get the same batches all the time \n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        # loop over all of memory \n",
    "        # for each vatchIndex, sample a whole batch of # samples -> train \n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            # take a sample from memory\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            # transpose our sample: list of tuples -> list of states, list of policies, list of targets \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            # change into numpy array\n",
    "            # .reshape(-1, 1): list of float values -> each of these value actually in own sub-array --> easy to compare with the output of model\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            # change into tensors \n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            # get policy, value from model by letting it predict the state \n",
    "            # purpose: reduce the different between (policy, mcst-distribution) and (value, final reward)\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            # need to minimize this loss by backpropagation \n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # pytorch does all big propagation for us \n",
    "            # instead of setting to 0, set the grads to None -> improve performance\n",
    "            # did not receive a gradient \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            # the gradients are stored by tensor themselves once call backward() on loss\n",
    "            # accumulates the gradient for each parameter \n",
    "            loss.backward()\n",
    "            # makes the optimiser iterate over all tensors -> update and use internally store grad to update their values \n",
    "            optimizer.step() # change to self.optimizer\n",
    "    \n",
    "    # use when we want to start the cycle of continuous learning\n",
    "    # where we run sort of play -> get data -> use data to train and optimize model\n",
    "    # close if play agian with the model that is much smarter \n",
    "    def learn(self):\n",
    "        # loop all iterations \n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            # each iter: store training data \n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            # loop over all self-play games\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                # for each iter -> extend data got out of selfPlay method \n",
    "                memory += self.selfPlay()\n",
    "            \n",
    "            # training part\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # store weights (learnable para) of our model, a dictionary object that maps each layer to its parameter tensor \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            # store static of optimizer \n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7f18c1b6474ffcbd85aa26ce6863ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f6d86f2e774feeb0d2210e7e55374b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeeabe759bd4f349088f63f736a003a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d79ec01ecdf4dc39a44b637ccfa4764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2a36bcdb82492cb4ce8f9e445e8e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade517ff9a844a689d5875fa48016160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c470145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [1, 2, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [ 1. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 1. -1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a396c8-f574-4efa-89f5-2e27dc555142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
